{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6cc7f3bbbc874cc080e948d56af33118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bad9ffdb9ba248678562e21e6c7db6ea",
              "IPY_MODEL_fc9117f009bc44cd9ee8efa4e0c29bef",
              "IPY_MODEL_d454d4e5558042088f0895a428d2a6d6"
            ],
            "layout": "IPY_MODEL_75c114e523734d79adf31450430fb324"
          }
        },
        "bad9ffdb9ba248678562e21e6c7db6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a142deddd5c42d0a62257c5cd5e4438",
            "placeholder": "​",
            "style": "IPY_MODEL_f092aa2726584deeb20e39cd4d8d3654",
            "value": "config.json: 100%"
          }
        },
        "fc9117f009bc44cd9ee8efa4e0c29bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bb4f8f373c84ef89128fbd633852c8f",
            "max": 6884,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70f60bbdb9944288b8e891c33c4856d5",
            "value": 6884
          }
        },
        "d454d4e5558042088f0895a428d2a6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c308023efe3d4182b18a78fcd5faa8ec",
            "placeholder": "​",
            "style": "IPY_MODEL_0873752874514e1fa934fe1d3a2759fb",
            "value": " 6.88k/6.88k [00:00&lt;00:00, 396kB/s]"
          }
        },
        "75c114e523734d79adf31450430fb324": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a142deddd5c42d0a62257c5cd5e4438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f092aa2726584deeb20e39cd4d8d3654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bb4f8f373c84ef89128fbd633852c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70f60bbdb9944288b8e891c33c4856d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c308023efe3d4182b18a78fcd5faa8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0873752874514e1fa934fe1d3a2759fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a9e600958544f12a8f4f991d0300e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_deac0ccbacc548d9aebda602053b1600",
              "IPY_MODEL_9a29dced579e4d14a83385ce7332570b",
              "IPY_MODEL_23b9962dcd2d44c29ed793112ff1e984"
            ],
            "layout": "IPY_MODEL_7a0ab21671f840198c6a7eb9b5482ce7"
          }
        },
        "deac0ccbacc548d9aebda602053b1600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5dbbf70621f4d6983ce248d430a9d47",
            "placeholder": "​",
            "style": "IPY_MODEL_bd5eedd77ee44140ba070b3196e3cedf",
            "value": "model.safetensors: 100%"
          }
        },
        "9a29dced579e4d14a83385ce7332570b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06c40b65be3f4b2da79a823df39ebb5c",
            "max": 15036944,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d2a8cb9b11f4126bc5538ce5b60fc9e",
            "value": 15036944
          }
        },
        "23b9962dcd2d44c29ed793112ff1e984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c379569f8a43ed8087d93a9667e119",
            "placeholder": "​",
            "style": "IPY_MODEL_dd69f1ca87df434cbe22e2b11d8d5e66",
            "value": " 15.0M/15.0M [00:00&lt;00:00, 91.3MB/s]"
          }
        },
        "7a0ab21671f840198c6a7eb9b5482ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5dbbf70621f4d6983ce248d430a9d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd5eedd77ee44140ba070b3196e3cedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06c40b65be3f4b2da79a823df39ebb5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d2a8cb9b11f4126bc5538ce5b60fc9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4c379569f8a43ed8087d93a9667e119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd69f1ca87df434cbe22e2b11d8d5e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gv7pPmKG_zH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3413dcbd-fbce-42d2-b465-91d2c625abe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install timm albumentations segmentation-models-pytorch --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "vndYAHbyNmnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e908263-698a-44ab-9cd3-4aaecb4e2187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n"
      ],
      "metadata": {
        "id": "uMYPPCQANqtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepGlobeDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, image_paths, mask_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(self.mask_paths[idx]))\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "# List paths\n",
        "image_dir = '/content/drive/MyDrive/archive (2)/data/data/training_data/images'\n",
        "mask_dir = '/content/drive/MyDrive/archive (2)/data/data/training_data/masks'\n",
        "\n",
        "image_paths = sorted([os.path.join(image_dir, x) for x in os.listdir(image_dir)])\n",
        "mask_paths = sorted([os.path.join(mask_dir, x) for x in os.listdir(mask_dir)])\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "dataset = DeepGlobeDataset(image_paths, mask_paths, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "q4tvOMcsNuPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "class BinarySegModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = smp.Unet(\n",
        "            encoder_name=\"resnet34\",\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=3,\n",
        "            classes=1,\n",
        "            activation=None\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "o6NKtTBnOLhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# RGB to class index map\n",
        "color_to_class = {\n",
        "    (0, 255, 255): 0,    # Urban\n",
        "    (255, 255, 0): 1,    # Agriculture\n",
        "    (255, 0, 255): 2,    # Rangeland\n",
        "    (0, 255, 0): 3,      # Forest\n",
        "    (0, 0, 255): 4,      # Water\n",
        "    (255, 255, 255): 5,  # Barren\n",
        "    (0, 0, 0): 6         # Unknown\n",
        "}\n",
        "\n",
        "# Input/output paths\n",
        "multi_class_mask_dir = '/content/drive/MyDrive/archive (2)/data/data/training_data/masks'\n",
        "output_dir = '/content/drive/MyDrive/archive (2)/data/data/training_data/binarymasks_rgb'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Process each image\n",
        "for fname in os.listdir(multi_class_mask_dir):\n",
        "    if not fname.endswith('.png'):\n",
        "        continue\n",
        "\n",
        "    mask = np.array(Image.open(os.path.join(multi_class_mask_dir, fname)))  # shape: (H, W, 3)\n",
        "\n",
        "    for rgb, class_idx in color_to_class.items():\n",
        "        binary_mask = np.all(mask == rgb, axis=-1).astype(np.uint8) * 255\n",
        "\n",
        "        out_path = os.path.join(output_dir, f\"{fname.replace('.png', '')}_class_{class_idx}.png\")\n",
        "        Image.fromarray(binary_mask).save(out_path)\n"
      ],
      "metadata": {
        "id": "E5fTZ21AHnf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Dataset Class ===\n",
        "class BinarySegDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"))\n",
        "        mask = (mask > 0).astype(np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"].unsqueeze(0)  # shape: (1, H, W)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# === Transform ===\n",
        "transform = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# === Paths ===\n",
        "image_dir = \"/content/drive/MyDrive/archive (2)/data/data/training_data/images\"\n",
        "binary_mask_dir = \"/content/drive/MyDrive/archive (2)/data/data/training_data/binarymasks_rgb\"\n",
        "\n",
        "# === Training Loop for Each Class ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 10\n",
        "num_classes = 7\n",
        "\n",
        "for class_id in range(num_classes):\n",
        "    print(f\"\\n🔁 Training binary segmentation model for Class {class_id}\\n\")\n",
        "\n",
        "    # === Collect image and class-specific mask paths ===\n",
        "    image_paths, mask_paths = [], []\n",
        "    for fname in os.listdir(image_dir):\n",
        "        if fname.endswith(\"_sat.jpg\"):\n",
        "            image_id = fname.replace(\"_sat.jpg\", \"\")\n",
        "            img_path = os.path.join(image_dir, fname)\n",
        "            mask_path = os.path.join(binary_mask_dir, f\"{image_id}_mask_class_{class_id}.png\")\n",
        "\n",
        "            if os.path.exists(mask_path):\n",
        "                image_paths.append(img_path)\n",
        "                mask_paths.append(mask_path)\n",
        "\n",
        "    if len(image_paths) == 0:\n",
        "        print(f\"⚠️ No masks found for class {class_id}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # === Split ===\n",
        "    train_img = image_paths[:int(0.8 * len(image_paths))]\n",
        "    train_msk = mask_paths[:int(0.8 * len(mask_paths))]\n",
        "    val_img = image_paths[int(0.8 * len(image_paths)):]\n",
        "    val_msk = mask_paths[int(0.8 * len(mask_paths)):]\n",
        "\n",
        "    # === Dataset & Dataloader ===\n",
        "    train_ds = BinarySegDataset(train_img, train_msk, transform)\n",
        "    val_ds = BinarySegDataset(val_img, val_msk, transform)\n",
        "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=4)\n",
        "\n",
        "    # === Model ===\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        num_labels=1,\n",
        "        ignore_mismatched_sizes=True\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # === Training Loop ===\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        loop = tqdm(train_loader, desc=f\"[Class {class_id}] Epoch [{epoch+1}/{EPOCHS}]\")\n",
        "\n",
        "        for images, masks in loop:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(pixel_values=images).logits\n",
        "            outputs = F.interpolate(outputs, size=masks.shape[2:], mode=\"bilinear\", align_corners=False).squeeze(1)\n",
        "            loss = criterion(outputs, masks.squeeze(1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"✅ [Class {class_id}] Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # === Save Model ===\n",
        "    save_dir = \"/content/drive/MyDrive/binary_seg_models\"  # Folder in your Drive\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
        "\n",
        "    model_path = os.path.join(save_dir, f\"binary_segformer_class_{class_id}.pth\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"✅ Saved model for class {class_id} at {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6cc7f3bbbc874cc080e948d56af33118",
            "bad9ffdb9ba248678562e21e6c7db6ea",
            "fc9117f009bc44cd9ee8efa4e0c29bef",
            "d454d4e5558042088f0895a428d2a6d6",
            "75c114e523734d79adf31450430fb324",
            "0a142deddd5c42d0a62257c5cd5e4438",
            "f092aa2726584deeb20e39cd4d8d3654",
            "7bb4f8f373c84ef89128fbd633852c8f",
            "70f60bbdb9944288b8e891c33c4856d5",
            "c308023efe3d4182b18a78fcd5faa8ec",
            "0873752874514e1fa934fe1d3a2759fb",
            "8a9e600958544f12a8f4f991d0300e34",
            "deac0ccbacc548d9aebda602053b1600",
            "9a29dced579e4d14a83385ce7332570b",
            "23b9962dcd2d44c29ed793112ff1e984",
            "7a0ab21671f840198c6a7eb9b5482ce7",
            "b5dbbf70621f4d6983ce248d430a9d47",
            "bd5eedd77ee44140ba070b3196e3cedf",
            "06c40b65be3f4b2da79a823df39ebb5c",
            "6d2a8cb9b11f4126bc5538ce5b60fc9e",
            "b4c379569f8a43ed8087d93a9667e119",
            "dd69f1ca87df434cbe22e2b11d8d5e66"
          ]
        },
        "id": "0xM8F-ujO6PK",
        "outputId": "e76f0582-218c-4f46-fe86-5c249415e951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Training binary segmentation model for Class 0\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/6.88k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cc7f3bbbc874cc080e948d56af33118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/15.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a9e600958544f12a8f4f991d0300e34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Class 0] Epoch [1/10]: 100%|██████████| 137/137 [02:39<00:00,  1.16s/it, loss=0.162]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 1 - Loss: 0.3253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [2/10]: 100%|██████████| 137/137 [01:38<00:00,  1.39it/s, loss=0.142]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 2 - Loss: 0.1797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [3/10]: 100%|██████████| 137/137 [01:38<00:00,  1.39it/s, loss=0.054]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 3 - Loss: 0.1291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [4/10]: 100%|██████████| 137/137 [01:39<00:00,  1.38it/s, loss=0.182]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 4 - Loss: 0.1415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [5/10]: 100%|██████████| 137/137 [01:39<00:00,  1.38it/s, loss=0.244]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 5 - Loss: 0.1003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [6/10]: 100%|██████████| 137/137 [01:38<00:00,  1.39it/s, loss=0.0469]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 6 - Loss: 0.1065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [7/10]: 100%|██████████| 137/137 [01:39<00:00,  1.37it/s, loss=0.0263]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 7 - Loss: 0.1125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [8/10]: 100%|██████████| 137/137 [01:39<00:00,  1.38it/s, loss=0.0221]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 8 - Loss: 0.0903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [9/10]: 100%|██████████| 137/137 [01:40<00:00,  1.36it/s, loss=0.189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 9 - Loss: 0.0832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 0] Epoch [10/10]: 100%|██████████| 137/137 [01:39<00:00,  1.38it/s, loss=0.115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 0] Epoch 10 - Loss: 0.0778\n",
            "✅ Saved model for class 0 at /content/drive/MyDrive/binary_seg_models/binary_segformer_class_0.pth\n",
            "\n",
            "🔁 Training binary segmentation model for Class 1\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Class 1] Epoch [1/10]: 100%|██████████| 137/137 [01:37<00:00,  1.40it/s, loss=0.749]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 1 - Loss: 0.5027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [2/10]: 100%|██████████| 137/137 [01:35<00:00,  1.43it/s, loss=0.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 2 - Loss: 0.4039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [3/10]: 100%|██████████| 137/137 [01:34<00:00,  1.46it/s, loss=0.394]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 3 - Loss: 0.3514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [4/10]: 100%|██████████| 137/137 [01:34<00:00,  1.44it/s, loss=0.255]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 4 - Loss: 0.3510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [5/10]: 100%|██████████| 137/137 [01:34<00:00,  1.45it/s, loss=0.549]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 5 - Loss: 0.3361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [6/10]: 100%|██████████| 137/137 [01:33<00:00,  1.47it/s, loss=0.341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 6 - Loss: 0.3008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [7/10]: 100%|██████████| 137/137 [01:33<00:00,  1.47it/s, loss=0.083]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 7 - Loss: 0.2669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [8/10]: 100%|██████████| 137/137 [01:33<00:00,  1.47it/s, loss=0.541]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 8 - Loss: 0.2579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [9/10]: 100%|██████████| 137/137 [01:33<00:00,  1.46it/s, loss=0.232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 9 - Loss: 0.2418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 1] Epoch [10/10]: 100%|██████████| 137/137 [01:34<00:00,  1.46it/s, loss=0.341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 1] Epoch 10 - Loss: 0.2105\n",
            "✅ Saved model for class 1 at /content/drive/MyDrive/binary_seg_models/binary_segformer_class_1.pth\n",
            "\n",
            "🔁 Training binary segmentation model for Class 2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Class 2] Epoch [1/10]: 100%|██████████| 137/137 [01:35<00:00,  1.44it/s, loss=0.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 1 - Loss: 0.3760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [2/10]: 100%|██████████| 137/137 [01:32<00:00,  1.48it/s, loss=0.863]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 2 - Loss: 0.2981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [3/10]: 100%|██████████| 137/137 [01:33<00:00,  1.46it/s, loss=0.748]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 3 - Loss: 0.2863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [4/10]: 100%|██████████| 137/137 [01:34<00:00,  1.45it/s, loss=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 4 - Loss: 0.2807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [5/10]: 100%|██████████| 137/137 [01:34<00:00,  1.44it/s, loss=0.131]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 5 - Loss: 0.2640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [6/10]: 100%|██████████| 137/137 [01:34<00:00,  1.45it/s, loss=0.151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 6 - Loss: 0.2634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [7/10]: 100%|██████████| 137/137 [01:34<00:00,  1.45it/s, loss=0.209]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 7 - Loss: 0.2515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [8/10]: 100%|██████████| 137/137 [01:34<00:00,  1.46it/s, loss=0.895]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 8 - Loss: 0.2371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [9/10]: 100%|██████████| 137/137 [01:35<00:00,  1.44it/s, loss=0.0584]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 9 - Loss: 0.2180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 2] Epoch [10/10]: 100%|██████████| 137/137 [01:35<00:00,  1.43it/s, loss=0.0519]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 2] Epoch 10 - Loss: 0.2102\n",
            "✅ Saved model for class 2 at /content/drive/MyDrive/binary_seg_models/binary_segformer_class_2.pth\n",
            "\n",
            "🔁 Training binary segmentation model for Class 3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Class 3] Epoch [1/10]: 100%|██████████| 137/137 [01:29<00:00,  1.52it/s, loss=0.219]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 1 - Loss: 0.3529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [2/10]: 100%|██████████| 137/137 [01:27<00:00,  1.57it/s, loss=0.177]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 2 - Loss: 0.1767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [3/10]: 100%|██████████| 137/137 [01:27<00:00,  1.57it/s, loss=0.0555]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 3 - Loss: 0.1391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [4/10]: 100%|██████████| 137/137 [01:26<00:00,  1.58it/s, loss=0.0436]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 4 - Loss: 0.1214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [5/10]: 100%|██████████| 137/137 [01:28<00:00,  1.56it/s, loss=0.0286]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 5 - Loss: 0.1134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [6/10]: 100%|██████████| 137/137 [01:27<00:00,  1.56it/s, loss=0.024]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 6 - Loss: 0.1093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [7/10]: 100%|██████████| 137/137 [01:27<00:00,  1.56it/s, loss=0.232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 7 - Loss: 0.1029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [8/10]: 100%|██████████| 137/137 [01:27<00:00,  1.56it/s, loss=0.0722]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 8 - Loss: 0.0834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [9/10]: 100%|██████████| 137/137 [01:28<00:00,  1.54it/s, loss=0.0117]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 9 - Loss: 0.0908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 3] Epoch [10/10]: 100%|██████████| 137/137 [01:27<00:00,  1.57it/s, loss=0.0184]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 3] Epoch 10 - Loss: 0.0671\n",
            "✅ Saved model for class 3 at /content/drive/MyDrive/binary_seg_models/binary_segformer_class_3.pth\n",
            "\n",
            "🔁 Training binary segmentation model for Class 4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Class 4] Epoch [1/10]: 100%|██████████| 137/137 [01:31<00:00,  1.50it/s, loss=0.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 1 - Loss: 0.3337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [2/10]: 100%|██████████| 137/137 [01:29<00:00,  1.53it/s, loss=0.087]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 2 - Loss: 0.1202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [3/10]: 100%|██████████| 137/137 [01:29<00:00,  1.54it/s, loss=0.0438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 3 - Loss: 0.0895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [4/10]: 100%|██████████| 137/137 [01:29<00:00,  1.52it/s, loss=0.0195]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 4 - Loss: 0.0746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [5/10]: 100%|██████████| 137/137 [01:30<00:00,  1.51it/s, loss=0.0849]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 5 - Loss: 0.0632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [6/10]: 100%|██████████| 137/137 [01:30<00:00,  1.51it/s, loss=0.0218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 6 - Loss: 0.0532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [7/10]: 100%|██████████| 137/137 [01:29<00:00,  1.54it/s, loss=0.0281]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 7 - Loss: 0.0459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [8/10]: 100%|██████████| 137/137 [01:30<00:00,  1.51it/s, loss=0.054]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 8 - Loss: 0.0415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [9/10]: 100%|██████████| 137/137 [01:30<00:00,  1.52it/s, loss=0.0553]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 9 - Loss: 0.0454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 4] Epoch [10/10]: 100%|██████████| 137/137 [01:29<00:00,  1.53it/s, loss=0.373]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 4] Epoch 10 - Loss: 0.0442\n",
            "✅ Saved model for class 4 at /content/drive/MyDrive/binary_seg_models/binary_segformer_class_4.pth\n",
            "\n",
            "🔁 Training binary segmentation model for Class 5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Class 5] Epoch [1/10]: 100%|██████████| 137/137 [01:32<00:00,  1.49it/s, loss=0.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 1 - Loss: 0.3788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [2/10]: 100%|██████████| 137/137 [01:29<00:00,  1.53it/s, loss=0.111]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 2 - Loss: 0.2414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [3/10]: 100%|██████████| 137/137 [01:29<00:00,  1.53it/s, loss=0.0673]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 3 - Loss: 0.2246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [4/10]: 100%|██████████| 137/137 [01:28<00:00,  1.55it/s, loss=0.0385]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 4 - Loss: 0.1892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [5/10]: 100%|██████████| 137/137 [01:29<00:00,  1.53it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 5 - Loss: 0.1738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [6/10]: 100%|██████████| 137/137 [01:29<00:00,  1.54it/s, loss=0.218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 6 - Loss: 0.1630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [7/10]: 100%|██████████| 137/137 [01:28<00:00,  1.55it/s, loss=0.0621]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 7 - Loss: 0.1330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [8/10]: 100%|██████████| 137/137 [01:29<00:00,  1.53it/s, loss=0.118]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 8 - Loss: 0.1387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [9/10]: 100%|██████████| 137/137 [01:30<00:00,  1.52it/s, loss=0.064]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 9 - Loss: 0.1120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 5] Epoch [10/10]: 100%|██████████| 137/137 [01:29<00:00,  1.53it/s, loss=0.0343]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 5] Epoch 10 - Loss: 0.1030\n",
            "✅ Saved model for class 5 at /content/drive/MyDrive/binary_seg_models/binary_segformer_class_5.pth\n",
            "\n",
            "🔁 Training binary segmentation model for Class 6\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Class 6] Epoch [1/10]: 100%|██████████| 137/137 [01:31<00:00,  1.50it/s, loss=0.0986]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 1 - Loss: 0.3339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [2/10]: 100%|██████████| 137/137 [01:27<00:00,  1.57it/s, loss=0.0495]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 2 - Loss: 0.0774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [3/10]: 100%|██████████| 137/137 [01:26<00:00,  1.58it/s, loss=0.0261]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 3 - Loss: 0.0354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [4/10]: 100%|██████████| 137/137 [01:28<00:00,  1.55it/s, loss=0.0158]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 4 - Loss: 0.0208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [5/10]: 100%|██████████| 137/137 [01:27<00:00,  1.57it/s, loss=0.0104]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 5 - Loss: 0.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [6/10]: 100%|██████████| 137/137 [01:28<00:00,  1.55it/s, loss=0.00791]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 6 - Loss: 0.0109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [7/10]: 100%|██████████| 137/137 [01:27<00:00,  1.57it/s, loss=0.00342]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 7 - Loss: 0.0078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [8/10]: 100%|██████████| 137/137 [01:28<00:00,  1.54it/s, loss=0.0091]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 8 - Loss: 0.0073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [9/10]: 100%|██████████| 137/137 [01:32<00:00,  1.49it/s, loss=0.00443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 9 - Loss: 0.0061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Class 6] Epoch [10/10]: 100%|██████████| 137/137 [01:33<00:00,  1.47it/s, loss=0.00391]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ [Class 6] Epoch 10 - Loss: 0.0058\n",
            "✅ Saved model for class 6 at /content/drive/MyDrive/binary_seg_models/binary_segformer_class_6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Dice and IoU functions ===\n",
        "def dice_score(pred, target, eps=1e-7):\n",
        "    pred = (pred > 0.5).float()\n",
        "    target = (target > 0.5).float()\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum()\n",
        "    dice = (2. * intersection + eps) / (union + eps)\n",
        "    return dice.item()\n",
        "\n",
        "def iou_score(pred, target, eps=1e-7):\n",
        "    pred = (pred > 0.5).float()\n",
        "    target = (target > 0.5).float()\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum() - intersection\n",
        "    iou = (intersection + eps) / (union + eps)\n",
        "    return iou.item()\n",
        "\n",
        "# === Constants ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "save_dir = \"/content/drive/MyDrive/binary_seg_models\"\n",
        "num_classes = 7\n",
        "\n",
        "# === Store results ===\n",
        "results = []\n",
        "\n",
        "# === Loop over all 7 classes ===\n",
        "for class_id in range(num_classes):\n",
        "    print(f\"\\n📊 Evaluating Model for Class {class_id}\")\n",
        "\n",
        "    # === Load validation dataset for current class ===\n",
        "    val_img = []\n",
        "    val_msk = []\n",
        "\n",
        "    for fname in os.listdir(image_dir):\n",
        "        if fname.endswith(\"_sat.jpg\"):\n",
        "            image_id = fname.replace(\"_sat.jpg\", \"\")\n",
        "            mask_path = os.path.join(binary_mask_dir, f\"{image_id}_mask_class_{class_id}.png\")\n",
        "            if os.path.exists(mask_path):\n",
        "                val_img.append(os.path.join(image_dir, fname))\n",
        "                val_msk.append(mask_path)\n",
        "\n",
        "    val_img = val_img[int(0.8 * len(val_img)):]\n",
        "    val_msk = val_msk[int(0.8 * len(val_msk)):]\n",
        "\n",
        "    val_ds = BinarySegDataset(val_img, val_msk, transform)\n",
        "    val_loader = DataLoader(val_ds, batch_size=4)\n",
        "\n",
        "    # === Load model ===\n",
        "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "        \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "        num_labels=1,\n",
        "        ignore_mismatched_sizes=True\n",
        "    ).to(device)\n",
        "\n",
        "    model_path = os.path.join(save_dir, f\"binary_segformer_class_{class_id}.pth\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # === Evaluate ===\n",
        "    dice_scores = []\n",
        "    iou_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            outputs = model(pixel_values=images).logits\n",
        "            outputs = F.interpolate(outputs, size=masks.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "\n",
        "            for pred, gt in zip(outputs, masks):\n",
        "                dice_scores.append(dice_score(pred, gt))\n",
        "                iou_scores.append(iou_score(pred, gt))\n",
        "\n",
        "    avg_dice = np.mean(dice_scores)\n",
        "    avg_iou = np.mean(iou_scores)\n",
        "\n",
        "    print(f\"✅ Class {class_id}: Dice = {avg_dice:.4f}, IoU = {avg_iou:.4f}\")\n",
        "    results.append((class_id, avg_dice, avg_iou))\n",
        "\n",
        "# === Optional: Save results as CSV ===\n",
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=[\"Class\", \"Dice Score\", \"IoU\"])\n",
        "results_df.to_csv(\"/content/drive/MyDrive/binary_seg_eval_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp7IPBnCoZdZ",
        "outputId": "47e82d72-7aed-4c9d-a293-4266a1e616b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluating Model for Class 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class 0: Dice = 0.5712, IoU = 0.4965\n",
            "\n",
            "📊 Evaluating Model for Class 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class 1: Dice = 0.7668, IoU = 0.7125\n",
            "\n",
            "📊 Evaluating Model for Class 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class 2: Dice = 0.3838, IoU = 0.3791\n",
            "\n",
            "📊 Evaluating Model for Class 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class 3: Dice = 0.8517, IoU = 0.8423\n",
            "\n",
            "📊 Evaluating Model for Class 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class 4: Dice = 0.5060, IoU = 0.4973\n",
            "\n",
            "📊 Evaluating Model for Class 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class 5: Dice = 0.5482, IoU = 0.5192\n",
            "\n",
            "📊 Evaluating Model for Class 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class 6: Dice = 0.8467, IoU = 0.8467\n"
          ]
        }
      ]
    }
  ]
}